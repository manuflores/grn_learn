{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional prediction of hypothetical TFs in bacteria using supervised machine learning in *E. coli* K-12 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Emanuel Flores-Bautista in 2018.  All code contained in this notebook is licensed under the [Creative Commons License 4.0](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:51:53.542615Z",
     "start_time": "2019-07-25T00:51:46.996044Z"
    }
   },
   "outputs": [],
   "source": [
    "##Import modules, community is the module for clustering networks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import random \n",
    "import community\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler as st\n",
    "import sort_seq as ss\n",
    "from sort_seq import * \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ss.set_plotting_style_2()\n",
    "##Setting the pyplot figures inside the notebook\n",
    "%matplotlib inline\n",
    "#Get svg graphics from the notebook\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:51:53.587777Z",
     "start_time": "2019-07-25T00:51:53.579493Z"
    }
   },
   "outputs": [],
   "source": [
    "path =  '../../../Documents/uni/bioinfo/data/coli/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TF-TF network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we saw that our approach works, let's work with the TF-TF network. Working with this subset allows us to analize the layer of the TRN that coordinates the TRN's dynamics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:51:53.709639Z",
     "start_time": "2019-07-25T00:51:53.636895Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading the TF-TF TRN, available at RegulonDB\n",
    "\n",
    "tf_trn = pd.read_csv(path + \"tf-tf-l.txt\", delimiter= '\\t', comment= '#', index_col= False)\n",
    "tf_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:51:53.827632Z",
     "start_time": "2019-07-25T00:51:53.816693Z"
    }
   },
   "outputs": [],
   "source": [
    "del(tf_trn['Unnamed: 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.describe()` method to have an overview of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:51:53.976432Z",
     "start_time": "2019-07-25T00:51:53.896511Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_trn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 456 interactions in the TF- TF network that CRP has the most outgoing arrows and GadX as the node with the most incoming arrows. Close to 70% of the TF-TF network has strong evidences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:00.894810Z",
     "start_time": "2019-07-25T00:52:00.884350Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's turn the TF TRN dataframe into a graph object\n",
    "net = nx.from_pandas_edgelist(df= tf_trn, source= 'TF', target='TG',\n",
    "                             edge_attr='regType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the global regulators (hubs) of the TF-TF network of $E. coli$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:01.440903Z",
     "start_time": "2019-07-25T00:52:01.411361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Computing the eigencentrality metric on the TF-TF net to get the hubs\n",
    "\n",
    "tf_hubs = get_network_hubs(net)\n",
    "\n",
    "tf_hubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the hubs of the TF-TF network is very similar to the complete TRN. This demonstrates that there are global regulators, but also local regulators and that hubs regulate both local TFs and TGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are some outliers in the network that are only regulating themselves (PAR, NAR), or regulating one gene (toxin-antitoxin). Let's extract the TF-TF network's largest connected component (LCC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:02.568062Z",
     "start_time": "2019-07-25T00:52:02.539140Z"
    }
   },
   "outputs": [],
   "source": [
    "##Computing the LCC\n",
    "net= max(nx.connected_component_subgraphs(net), key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it using the `draw()` function of Nx. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-TF Network Clustering using the Louvain algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cluster the TF-TF network's LCC using the Louvain algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:04.077922Z",
     "start_time": "2019-07-25T00:52:03.902691Z"
    }
   },
   "outputs": [],
   "source": [
    "##Cluster the TF-TF network LCC\n",
    "\n",
    "communities = community.best_partition(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:04.171774Z",
     "start_time": "2019-07-25T00:52:04.161568Z"
    }
   },
   "outputs": [],
   "source": [
    "n_clusters_tf = max(communities.values())\n",
    "\n",
    "n_clusters_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 11 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:04.711057Z",
     "start_time": "2019-07-25T00:52:04.701776Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Let's look at the cluster assignment for each TF in the TF-TF network. \n",
    "#To do this remove the hash(#) before communities.\n",
    "\n",
    "#communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OxyR in Cluster 11, SoxRS/ MarAR in cluster 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this cluster labels as an attribute in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:05.656604Z",
     "start_time": "2019-07-25T00:52:05.651870Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.set_node_attributes(net, values= communities, name='modularity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Expression data pre-processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make the pre-processing of the expression data, let's extract the data corresponding to the hypothetical TFs, we'll later use it to make our predictions, i.e. to test what is the most likely functional role of each putative TF inside the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:06.565843Z",
     "start_time": "2019-07-25T00:52:06.555744Z"
    }
   },
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:07.014244Z",
     "start_time": "2019-07-25T00:52:07.003007Z"
    }
   },
   "outputs": [],
   "source": [
    "tf = pd.read_csv(path + 'tf_list_gene_name.csv', comment = '#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:07.260228Z",
     "start_time": "2019-07-25T00:52:07.247773Z"
    }
   },
   "outputs": [],
   "source": [
    "hyp_tfs = pd.read_csv(path + 'hypTF_list_genes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:07.487622Z",
     "start_time": "2019-07-25T00:52:07.473549Z"
    }
   },
   "outputs": [],
   "source": [
    "hyp_tfs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:07.720772Z",
     "start_time": "2019-07-25T00:52:07.709678Z"
    }
   },
   "outputs": [],
   "source": [
    "hypTFs = hyp_tfs.hyptfs.values\n",
    "\n",
    "#tf_list = tf.TF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:12.581731Z",
     "start_time": "2019-07-25T00:52:12.573823Z"
    }
   },
   "outputs": [],
   "source": [
    "'yjjj' in hypTFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:14.038026Z",
     "start_time": "2019-07-25T00:52:14.029120Z"
    }
   },
   "outputs": [],
   "source": [
    "len(hypTFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:53:55.329836Z",
     "start_time": "2019-07-25T00:53:55.325826Z"
    }
   },
   "outputs": [],
   "source": [
    "hypTFs = list(hypTFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Raw expression data from Colombos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:29.613854Z",
     "start_time": "2019-07-25T00:52:21.618296Z"
    }
   },
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(path + \"ecoli_exp_data_COLOMBOS.txt\", delimiter= '\\t', comment= '#')\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:56:06.657588Z",
     "start_time": "2019-07-25T00:56:06.645328Z"
    }
   },
   "outputs": [],
   "source": [
    "df_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:55:21.940840Z",
     "start_time": "2019-07-25T00:54:09.175784Z"
    }
   },
   "outputs": [],
   "source": [
    "annot, denoised_df = exp_data_preprocessing(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:56:14.528286Z",
     "start_time": "2019-07-25T00:56:14.517173Z"
    }
   },
   "outputs": [],
   "source": [
    "denoised_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:56:33.543059Z",
     "start_time": "2019-07-25T00:56:33.163152Z"
    }
   },
   "outputs": [],
   "source": [
    "full_denoised_data = pd.concat([annot, denoised_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:56:38.373626Z",
     "start_time": "2019-07-25T00:56:38.337832Z"
    }
   },
   "outputs": [],
   "source": [
    "full_denoised_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:30:00.022915Z",
     "start_time": "2019-07-25T01:30:00.014891Z"
    }
   },
   "outputs": [],
   "source": [
    "full_denoised_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the experimental and hypothetical TFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with our workflow, we have to separate the experimental and hypothetical TFs. This step has two advantages. First, we can train our network with only experimental TFs, and second, we can then use this HypTF dataset to make the functional predictions in one step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T00:52:31.321557Z",
     "start_time": "2019-07-25T00:52:31.311329Z"
    }
   },
   "outputs": [],
   "source": [
    "hypTFs.extend(['dgor', 'ykfn', 'frlr', 'bdcr', 'mqsa', 'fimz', 'sgcr', 'dlmr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:04:16.952937Z",
     "start_time": "2019-07-25T01:04:16.889385Z"
    }
   },
   "outputs": [],
   "source": [
    "hypTFs_list = []\n",
    "\n",
    "for row in full_denoised_data['gene name']:\n",
    "    if row in hypTFs:\n",
    "        hypTFs_list.append(1)\n",
    "    else:\n",
    "        hypTFs_list.append(0) \n",
    "        \n",
    "full_denoised_data['hyp'] = hypTFs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:05:40.254466Z",
     "start_time": "2019-07-25T01:05:40.194314Z"
    }
   },
   "outputs": [],
   "source": [
    "##Filtering out the HypTFs\n",
    "\n",
    "hyp_tfs_test = full_denoised_data[full_denoised_data['hyp'] ==1]\n",
    "hyp_tfs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:05:44.579953Z",
     "start_time": "2019-07-25T01:05:44.570536Z"
    }
   },
   "outputs": [],
   "source": [
    "hyp_tfs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:05:48.041027Z",
     "start_time": "2019-07-25T01:05:48.032629Z"
    }
   },
   "outputs": [],
   "source": [
    "del(hyp_tfs_test['hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:05:50.050385Z",
     "start_time": "2019-07-25T01:05:50.013894Z"
    }
   },
   "outputs": [],
   "source": [
    "hyp_tfs_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:06:18.471203Z",
     "start_time": "2019-07-25T01:06:16.965433Z"
    }
   },
   "outputs": [],
   "source": [
    "hyp_tfs_test.to_csv('../data/ml_dfs/hyp_tfs_coli_X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre process expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:06:31.476516Z",
     "start_time": "2019-07-25T01:06:31.335040Z"
    }
   },
   "outputs": [],
   "source": [
    "##Filtering out all of the genes that aren't HypTFs\n",
    "## We'll use this dataframe for downstream analysis\n",
    "\n",
    "df_xx = full_denoised_data[full_denoised_data['hyp'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:06:34.047940Z",
     "start_time": "2019-07-25T01:06:34.010905Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:06:45.620211Z",
     "start_time": "2019-07-25T01:06:45.609377Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:06:40.570681Z",
     "start_time": "2019-07-25T01:06:40.561880Z"
    }
   },
   "outputs": [],
   "source": [
    "del(df_xx['hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:07:09.429694Z",
     "start_time": "2019-07-25T01:07:09.395803Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:08:11.919227Z",
     "start_time": "2019-07-25T01:07:19.854079Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.to_csv('../data/ml_dfs/coli_denoised_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T23:10:05.989290Z",
     "start_time": "2018-12-19T17:10:05.984419-06:00"
    }
   },
   "source": [
    "from sklearn.feature_selection import SelectPercentile,\\\n",
    "mutual_info_classif, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification data preparation: Assign the cluster labels to the expression dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to make the classification. Sidenote: we'll try to not include the global regulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, let's extract the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:13.190983Z",
     "start_time": "2019-07-25T01:09:13.180582Z"
    }
   },
   "outputs": [],
   "source": [
    "n_clusters_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:16.441054Z",
     "start_time": "2019-07-25T01:09:16.432087Z"
    }
   },
   "outputs": [],
   "source": [
    "#tf_cluster_list = []\n",
    "tf_clusters = get_network_clusters(net, n_clusters_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:19.637487Z",
     "start_time": "2019-07-25T01:09:19.569178Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster1, cluster2, cluster3, cluster4,\\\n",
    "cluster5, cluster6, cluster7, cluster8, \\\n",
    "cluster9, cluster10, cluster11 = tf_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we'll filter out the regulons for each cluster, using the TRN data from RegulonDB, stored in the `trn_df` object. After that, we'll make a list of the TGs in each regulon, that we'll later use to set the labels of each cluster and then proceed to the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Notice that the clusters vary with each run. We will extract the regulons using a high confidence run, that extracted functionally robust TF clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1 : DNA repair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:35.793367Z",
     "start_time": "2019-07-25T01:09:35.771822Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv(path + 'trn-l.txt', sep = '\\t',\n",
    "                     comment = '#', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:38.664929Z",
     "start_time": "2019-07-25T01:09:38.640241Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:42.212577Z",
     "start_time": "2019-07-25T01:09:41.933587Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's check the TFs of cluster 1\n",
    "#print('Some members of cluster 1', cluster1[:5])\n",
    "\n",
    "#Now let's filter the regulons of each TF from the TRN\n",
    "\n",
    "cluster_1 = trn_df[ (trn_df['tf'] == 'dnaa') | (trn_df['tf'] == 'yedw')  | \\\n",
    "                  (trn_df['tf'] == 'ydfh') | (trn_df['tf'] == 'phob') \\\n",
    "                  | (trn_df['tf'] == 'cusr') | (trn_df['tf'] == 'argp') \\\n",
    "                  | (trn_df['tf'] == 'ascg') | (trn_df['tf'] == 'prpr')  ]\n",
    "cluster_1_tgs = []\n",
    "\n",
    "#Making a list that corresponds to the first cluster's target genes (TGs)\n",
    "\n",
    "for row in cluster_1['tg']:\n",
    "    cluster_1_tgs.append(row)\n",
    "    \n",
    "#Make a set to avoid repetition, and then re-make a list out of it.\n",
    "    \n",
    "cluster1_tgs = list(set(cluster_1_tgs))\n",
    "\n",
    "#print('Cluster 1 has {} nodes'.format(len(cluster1_tgs)))\n",
    "\n",
    "#Let's look at the TFs of cluster 2\n",
    "#print('Some members of cluster 2', cluster2[:5])\n",
    "\n",
    "#Filter the regulons of each TF in cluster 2 from the TRN\n",
    "cluster_2 =  trn_df[ (trn_df['tf'] == 'phop') |  (trn_df['tf'] == 'ydeo') \\\n",
    "                       | (trn_df['tf'] == 'rutr') | (trn_df['tf'] == 'gade')  \\\n",
    "                    | (trn_df['tf'] == 'lrp') | (trn_df['tf'] == 'stpa') \\\n",
    "                    | (trn_df['tf'] == 'rscb') | (trn_df['tf'] == 'gadw') \\\n",
    "                    | (trn_df['tf'] == 'h-ns') | (trn_df['tf'] == 'leuo')| (trn_df['tf'] == 'adiy')   | (trn_df['tf'] == 'evga') \\\n",
    "                   | (trn_df['tf'] == 'nemr') | (trn_df['tf'] == 'rcsb-bglj') \\\n",
    "                   | (trn_df['tf'] == 'trer') | (trn_df['tf'] == 'cspa') \\\n",
    "                   | (trn_df['tf'] == 'gadx') | (trn_df['tf'] == 'torr') \\\n",
    "                   | (trn_df['tf'] == 'hns')  | (trn_df['tf'] == 'nhar') \\\n",
    "                   | (trn_df['tf'] == 'bglj') | (trn_df['tf'] == 'sdia') \\\n",
    "                   | (trn_df['tf'] == 'rcsa') ]\n",
    "\n",
    "cluster_2_tgs = []\n",
    "\n",
    "#Making a list that corresponds to the second cluster's target genes (TGs)\n",
    "\n",
    "for row in cluster_2['tg']:\n",
    "    cluster_2_tgs.append(row)\n",
    "    \n",
    "    \n",
    "#Make a set to avoid repetition, and then re-make a list out of it.\n",
    "    \n",
    "cluster2_tgs = list(set(cluster_2_tgs))\n",
    "\n",
    "#Let's see how many TGs does cluster 2 have \n",
    "\n",
    "#print('Cluster 2 has {} nodes'.format(len(cluster2_tgs)))\n",
    "\n",
    "cluster_3 =  trn_df[(trn_df['tf'] == 'rob') | (trn_df['tf'] == 'soxr') \\\n",
    "                     |(trn_df['tf'] == 'acrr')| (trn_df['tf'] == 'soxs') \\\n",
    "                    |(trn_df['tf'] =='pdel' ) | (trn_df['tf'] == 'hupa')\\\n",
    "                    |(trn_df['tf'] =='mtlr' ) | (trn_df['tf'] =='hupb') \\\n",
    "                    |(trn_df['tf'] =='baer') | (trn_df['tf'] =='marr') \\\n",
    "                    |(trn_df['tf'] =='cra') \\\n",
    "                    |(trn_df['tf'] =='decr') |(trn_df['tf'] =='cpxr')\\\n",
    "                    |(trn_df['tf'] =='mara')]\n",
    "\n",
    "\n",
    "cluster_3_tgs = []\n",
    "\n",
    "#Making a list that corresponds to the cluster's target genes (TGs)\n",
    "\n",
    "for row in cluster_3['tg']:\n",
    "    cluster_3_tgs.append(row)\n",
    "    \n",
    "#Make a set to avoid repetition, and then re-make a list out of it.\n",
    "\n",
    "    \n",
    "cluster3_tgs = list(set(cluster_3_tgs))\n",
    "\n",
    "#print('Cluster 3 has {} nodes'.format(len(cluster3_tgs)))\n",
    "\n",
    "\n",
    "#Filter the regulons of each TF in cluster 4 from the TRN...\n",
    "\n",
    "cluster_4 =  trn_df[ (trn_df['tf'] == 'metj') | (trn_df['tf'] == 'fur') \\\n",
    "                    | (trn_df['tf'] == 'oxyr') | (trn_df['tf'] == 'purr') \\\n",
    "                    | (trn_df['tf'] =='metr' )  ]\n",
    "\n",
    "cluster_4_tgs = []\n",
    "\n",
    "#Making a list that corresponds to the cluster's target genes (TGs)...\n",
    "\n",
    "for row in cluster_4['tg']:\n",
    "    cluster_4_tgs.append(row)\n",
    "    \n",
    "#Make a set to avoid repetition, and then re-make a list out of it...\n",
    "cluster4_tgs = list(set(cluster_4_tgs))\n",
    "\n",
    "print('Cluster 4 has {} nodes'.format(len(cluster4_tgs)))\n",
    "\n",
    "\n",
    "cluster_5 =  trn_df[ (trn_df['tf'] == 'mata') | (trn_df['tf'] == 'csgd') \\\n",
    "                    | (trn_df['tf'] == 'mlra') | (trn_df['tf'] == 'puta') \\\n",
    "                    | (trn_df['tf'] =='rsta' ) | (trn_df['tf'] =='flhdc' )\\\n",
    "                    | (trn_df['tf'] =='ecpr' ) | (trn_df['tf'] =='mqsa' ) \\\n",
    "                    | (trn_df['tf'] =='flhc' ) | (trn_df['tf'] =='flhd' ) \\\n",
    "                    | (trn_df['tf'] =='sutr' ) | (trn_df['tf'] =='basr' ) \\\n",
    "                    | (trn_df['tf'] =='mqsr' ) | (trn_df['tf'] =='ompr' ) \\\n",
    "                    | (trn_df['tf'] =='bola' ) | (trn_df['tf'] =='rcda' ) \\\n",
    "                    | (trn_df['tf'] =='fliz' ) | (trn_df['tf'] =='hdfr' ) \\\n",
    "                    | (trn_df['tf'] =='cadc' ) | (trn_df['tf'] =='lrha' ) \\\n",
    "                    | (trn_df['tf'] =='yjjq' ) | (trn_df['tf'] =='qseb' ) ]\n",
    "\n",
    "cluster_5_tgs = []\n",
    "\n",
    "for row in cluster_5['tg']:\n",
    "    cluster_5_tgs.append(row)\n",
    "    \n",
    "cluster5_tgs = list(set(cluster_5_tgs))\n",
    "\n",
    "print('Cluster 5 has {} nodes'.format(len(cluster5_tgs)))\n",
    "\n",
    "\n",
    "cluster_6 =  trn_df[ (trn_df['tf'] == 'srlr') | (trn_df['tf'] == 'rbsr') \\\n",
    "                    | (trn_df['tf'] == 'zrar') | (trn_df['tf'] == 'mhpr') \\\n",
    "                    | (trn_df['tf'] =='malt' ) | (trn_df['tf'] =='mali' ) \\\n",
    "                    | (trn_df['tf'] =='gntr' ) | (trn_df['tf'] =='fucr' ) \\\n",
    "                    | (trn_df['tf'] =='uxur' ) | (trn_df['tf'] =='gutm' ) \\\n",
    "                    | (trn_df['tf'] =='mlc' ) | (trn_df['tf'] =='nagc' ) \\\n",
    "                    | (trn_df['tf'] =='exur' ) | (trn_df['tf'] =='melr' ) \\\n",
    "                    | (trn_df['tf'] =='lsrr' ) | (trn_df['tf'] =='cytr' ) \\\n",
    "                    | (trn_df['tf'] =='rhar' ) | (trn_df['tf'] =='idnr' ) \\\n",
    "                    | (trn_df['tf'] =='gutr' ) | (trn_df['tf'] =='comr' ) \\\n",
    "                    | (trn_df['tf'] =='glpr' ) | (trn_df['tf'] =='chbr' ) \\\n",
    "                    | (trn_df['tf'] =='creb' ) | (trn_df['tf'] =='laci' ) \\\n",
    "                    | (trn_df['tf'] =='rhas' ) \\\n",
    "                    \n",
    "                   ]\n",
    "\n",
    "\n",
    "cluster_6_tgs = []\n",
    "\n",
    "for row in cluster_6['tg']:\n",
    "    cluster_6_tgs.append(row)\n",
    "    \n",
    "cluster6_tgs = list(set(cluster_6_tgs))\n",
    "\n",
    "print('Cluster 6 has {} nodes'.format(len(cluster6_tgs)))\n",
    "\n",
    "cluster_7 =  trn_df[ (trn_df['tf'] == 'maze-mazf') | (trn_df['tf'] == 'tdcr') \\\n",
    "                    | (trn_df['tf'] == 'yeil') | (trn_df['tf'] == 'hipab') \\\n",
    "                    | (trn_df['tf'] =='ihfb' ) | (trn_df['tf'] =='tdca' ) \\\n",
    "                    | (trn_df['tf'] =='hipb' ) | (trn_df['tf'] =='hipa' ) \\\n",
    "                    | (trn_df['tf'] =='yiaj' ) \\\n",
    "                    | (trn_df['tf'] =='maze' )\n",
    "                   ]\n",
    "\n",
    "cluster_7_tgs = []\n",
    "\n",
    "for row in cluster_7['tg']:\n",
    "    cluster_7_tgs.append(row)\n",
    "    \n",
    "cluster7_tgs = list(set(cluster_7_tgs))\n",
    "\n",
    "print('Cluster 7 has {} nodes'.format(len(cluster7_tgs)))\n",
    "\n",
    "\n",
    "cluster_8 =  trn_df[ (trn_df['tf'] == 'puur') | (trn_df['tf'] == 'xylr') \\\n",
    "                    | (trn_df['tf'] == 'beti') | (trn_df['tf'] == 'lldr') \\\n",
    "                     | (trn_df['tf'] =='arac' )]\n",
    "\n",
    "cluster_8_tgs = []\n",
    "\n",
    "for row in cluster_8['tg']:\n",
    "    cluster_8_tgs.append(row)\n",
    "    \n",
    "cluster8_tgs = list(set(cluster_8_tgs))\n",
    "\n",
    "print('Cluster 8 has {} nodes'.format(len(cluster8_tgs)))\n",
    "\n",
    "\n",
    "cluster_9 =  trn_df[ (trn_df['tf'] == 'narl') | (trn_df['tf'] == 'pdhr') \\\n",
    "                    | (trn_df['tf'] == 'hyfr') | (trn_df['tf'] == 'fhla') \\\n",
    "                    | (trn_df['tf'] =='dcur' ) \\\n",
    "                    | (trn_df['tf'] =='mode' ) | (trn_df['tf'] =='caif' ) \\\n",
    "                    | (trn_df['tf'] =='nikr' ) | (trn_df['tf'] =='mraz' ) \\\n",
    "                    | (trn_df['tf'] =='dpia' ) | (trn_df['tf'] =='yqji' ) \\\n",
    "                    | (trn_df['tf'] =='appy' ) | (trn_df['tf'] =='sigma54' )]\n",
    "\n",
    "\n",
    "cluster_9_tgs = []\n",
    "\n",
    "for row in cluster_9['tg']:\n",
    "    cluster_9_tgs.append(row)\n",
    "    \n",
    "cluster9_tgs = list(set(cluster_9_tgs))\n",
    "\n",
    "print('Cluster 9 has {} nodes'.format(len(cluster9_tgs)))\n",
    "\n",
    "\n",
    "cluster_10 =  trn_df[ (trn_df['tf'] == 'norr') | (trn_df['tf'] == 'cbl') \\\n",
    "                    | (trn_df['tf'] == 'nsrr') | (trn_df['tf'] == 'fear') \\\n",
    "                    | (trn_df['tf'] =='ntrc' ) | (trn_df['tf'] =='cysb' ) \\\n",
    "                    | (trn_df['tf'] =='glng' ) | (trn_df['tf'] =='asnc' ) \\\n",
    "                    | (trn_df['tf'] =='dsdc' ) | (trn_df['tf'] =='ihfa' ) \\\n",
    "                    | (trn_df['tf'] =='nac' )  ]\n",
    "\n",
    "cluster_10_tgs = []\n",
    "\n",
    "for row in cluster_10['tg']:\n",
    "    cluster_10_tgs.append(row)\n",
    "    \n",
    "cluster10_tgs = list(set(cluster_10_tgs))\n",
    "\n",
    "print('Cluster 10 has {} nodes'.format(len(cluster10_tgs)))\n",
    "\n",
    "\n",
    "cluster_11 =  trn_df[ (trn_df['tf'] == 'ada') | (trn_df['tf'] == 'aidb') ]\n",
    "cluster_11_tgs = []\n",
    "\n",
    "for row in cluster_11['tg']:\n",
    "    cluster_11_tgs.append(row)\n",
    "    \n",
    "cluster11_tgs = list(set(cluster_11_tgs))\n",
    "\n",
    "print('Cluster 11 has {} nodes'.format(len(cluster11_tgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 2 : Glutamate dependent acid response(GDAR)\n",
    "\n",
    "Cluster 3: Multi-stress response, and global regulators\n",
    "\n",
    "Note: Let's not consider Fis protein.\n",
    "\n",
    "C4: Iron, purines, and ros response\n",
    "\n",
    "Cluster 5 : Biofilm and motility\n",
    "\n",
    "Cluster 6 : Central carbon metabolism. Let's not consider CRP. \n",
    "\n",
    "#| (trn_df['tf'] =='crp' )\n",
    "#| (trn_df['tf'] =='ihf' )\n",
    "#| (trn_df['tf'] =='arca' )\n",
    "#| (trn_df['tf'] =='fnr' ) \n",
    "\n",
    "Cluster 7: Toxin-antitoxin systems(TAS). Let's not consider IHF. \n",
    "\n",
    "Note-to-self: One can use the cluster dataframes to make subnetwork visualizations\n",
    "\n",
    "Cluster 8: Carbohydrate metabolism and respiration\n",
    "\n",
    "Let's not consider ArcA\n",
    "\n",
    "Cluster 9 : Nitrogen metabolism. Note: Let's not consider Fnr. \n",
    "\n",
    "Cluster 10 : aminoacid and\n",
    "\n",
    "Because, cluster 11 is so small, compared to the other clusters, and might generate an unbalanced training, we'll not consider it as part of the classification procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:46.699763Z",
     "start_time": "2019-07-25T01:09:46.666297Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's re-check our df_xx dataframe, that corresponds to the annot + exp data\n",
    "\n",
    "df_xx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with the classification procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:52.147049Z",
     "start_time": "2019-07-25T01:09:52.138930Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initializing the labels' lists\n",
    "\n",
    "labels1 = []\n",
    "labels2 = []\n",
    "labels3 = []\n",
    "labels4 = []\n",
    "labels5 = []\n",
    "labels6 = []\n",
    "labels7 = []\n",
    "labels8 = []\n",
    "labels9 = []\n",
    "labels10 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:54.890082Z",
     "start_time": "2019-07-25T01:09:54.661575Z"
    }
   },
   "outputs": [],
   "source": [
    "##Seting up the labels for each cluster\n",
    "\n",
    "#C1\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster1_tgs:\n",
    "        labels1.append(1)\n",
    "    else:\n",
    "        labels1.append(0)\n",
    "        \n",
    "#C2        \n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster2_tgs:\n",
    "        labels2.append(1)\n",
    "    else:\n",
    "        labels2.append(0)\n",
    "        \n",
    "#C3\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster3_tgs:\n",
    "        labels3.append(1)\n",
    "    else:\n",
    "        labels3.append(0)\n",
    "\n",
    "#C4 \n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster4_tgs:\n",
    "        labels4.append(1)\n",
    "    else:\n",
    "        labels4.append(0)\n",
    "\n",
    "#C5\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster5_tgs:\n",
    "        labels5.append(1)\n",
    "    else:\n",
    "        labels5.append(0)\n",
    "        \n",
    "#C6\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster6_tgs:\n",
    "        labels6.append(1)\n",
    "    else:\n",
    "        labels6.append(0)\n",
    "        \n",
    "#C7\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster7_tgs:\n",
    "        labels7.append(1)\n",
    "    else:\n",
    "        labels7.append(0)\n",
    "        \n",
    "#C8\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster8_tgs:\n",
    "        labels8.append(1)\n",
    "    else:\n",
    "        labels8.append(0)\n",
    "        \n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster9_tgs:\n",
    "        labels9.append(1)\n",
    "    else:\n",
    "        labels9.append(0)\n",
    "        \n",
    "#C10\n",
    "for row in df_xx['gene name']:\n",
    "    if row in cluster10_tgs:\n",
    "        labels10.append(1)\n",
    "    else:\n",
    "        labels10.append(0)\n",
    "        \n",
    "#C11\n",
    "#for row in df_xx['gene name']:\n",
    "#    if row in cluster11_tgs:\n",
    "#        labels11.append(1)\n",
    "#    else:\n",
    "#        labels11.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:57.338626Z",
     "start_time": "2019-07-25T01:09:57.323470Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking if we have correct classification annotation\n",
    "1 in labels3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's append this lists as columns in the dataframe, for each of the clusters' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:11:17.328947Z",
     "start_time": "2019-07-25T01:11:16.161833Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx['cluster 1'] = labels1\n",
    "df_xx['cluster 2'] = labels2\n",
    "df_xx['cluster 3'] = labels3\n",
    "df_xx['cluster 4'] = labels4\n",
    "df_xx['cluster 5'] = labels5\n",
    "df_xx['cluster 6'] = labels6\n",
    "df_xx['cluster 7'] = labels7\n",
    "df_xx['cluster 8'] = labels8\n",
    "df_xx['cluster 9'] = labels9\n",
    "df_xx['cluster 10'] = labels10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:12:10.330388Z",
     "start_time": "2019-07-25T01:12:10.296002Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we classified our data set correctly. Let's take SoxS as an example. Notice that with each run, the clusters change (b/c of the louvain algorithm), so this step has to be adapted with each run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:12:20.779209Z",
     "start_time": "2019-07-25T01:12:20.761949Z"
    }
   },
   "outputs": [],
   "source": [
    "annot[annot['gene name'] == ('soxs')]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the multiple stress response TFs are in cluster3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:12:49.325425Z",
     "start_time": "2019-07-25T01:12:49.316958Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have its location in the network and its cluster label, let's check it in the `df_exp` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:13:05.075976Z",
     "start_time": "2019-07-25T01:13:05.066782Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.loc[3784, 'cluster 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Voil√†.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if a random TF is correctly classified as a non-member of the cluster4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:21:37.209389Z",
     "start_time": "2019-07-24T08:21:37.197144Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.choice(list(communities.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:21:42.941235Z",
     "start_time": "2019-07-24T08:21:42.897198Z"
    }
   },
   "outputs": [],
   "source": [
    "'rcsb-bglj' in cluster3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:21:47.852124Z",
     "start_time": "2019-07-24T08:21:47.813366Z"
    }
   },
   "outputs": [],
   "source": [
    "annot[annot['gene name'] == ('rcsb')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:21:52.589446Z",
     "start_time": "2019-07-24T08:21:52.560750Z"
    }
   },
   "outputs": [],
   "source": [
    "denoised_df.loc[2083, 'cluster 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're good to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the training and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the current knowledge we have from the TRN, we will train a neural network with the known TFs and test the network to predict the label association of the hypothetical TFs. \n",
    "\n",
    "\n",
    "We'll make a partition with training data being the regulons + random noise (to avoid overfitting). The random noise will be the expression data for genes that do not appear to be regulated by TFs, as according with the RegulonDB TRN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:13:48.966958Z",
     "start_time": "2019-07-25T01:13:48.960024Z"
    }
   },
   "outputs": [],
   "source": [
    "tgs_set = set(cluster1_tgs+cluster2_tgs +cluster3_tgs+\n",
    "             cluster4_tgs + cluster5_tgs + cluster6_tgs + \n",
    "             cluster7_tgs + cluster8_tgs + cluster9_tgs+ \n",
    "             cluster10_tgs)\n",
    "\n",
    "TGs_list = [1 if row in tgs_set else 0 for row in df_xx['gene name'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:13:51.827099Z",
     "start_time": "2019-07-25T01:13:51.812730Z"
    }
   },
   "outputs": [],
   "source": [
    "len(TGs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:14:54.372041Z",
     "start_time": "2019-07-25T01:14:54.337743Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:14:59.210667Z",
     "start_time": "2019-07-25T01:14:59.101915Z"
    }
   },
   "outputs": [],
   "source": [
    "##Adding the TG list as a column to the expression data\n",
    "\n",
    "df_xx['TGs'] = TGs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:08.906124Z",
     "start_time": "2019-07-25T01:15:08.851060Z"
    }
   },
   "outputs": [],
   "source": [
    "##Let's filter out the genes that are regulated by TFs\n",
    "\n",
    "regulons_df = df_xx[df_xx['TGs'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:13.267070Z",
     "start_time": "2019-07-25T01:15:13.147643Z"
    }
   },
   "outputs": [],
   "source": [
    "regulons_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:17.515384Z",
     "start_time": "2019-07-25T01:15:17.502294Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's delete the TGs column\n",
    "\n",
    "del(regulons_df['TGs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:21.913122Z",
     "start_time": "2019-07-25T01:15:21.901211Z"
    }
   },
   "outputs": [],
   "source": [
    "regulons_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:30.727707Z",
     "start_time": "2019-07-25T01:15:30.626837Z"
    }
   },
   "outputs": [],
   "source": [
    "##Let's filter out the genes that are not regulated by TFs\n",
    "\n",
    "non_reg_df  = df_xx[df_xx['TGs'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:34.421300Z",
     "start_time": "2019-07-25T01:15:34.412863Z"
    }
   },
   "outputs": [],
   "source": [
    "non_reg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:37.777988Z",
     "start_time": "2019-07-25T01:15:37.763301Z"
    }
   },
   "outputs": [],
   "source": [
    "del(non_reg_df['TGs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:41.598797Z",
     "start_time": "2019-07-25T01:15:41.571947Z"
    }
   },
   "outputs": [],
   "source": [
    "##Making a dataframe called noise, by randomly picking \n",
    "##genes that are NOT REGULATED by TFs without replacement\n",
    "\n",
    "noise = non_reg_df.sample(n = 50, replace = False, axis = 0, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:45.943013Z",
     "start_time": "2019-07-25T01:15:45.902382Z"
    }
   },
   "outputs": [],
   "source": [
    "regulons_with_noise = pd.concat([regulons_df, noise]) ## unbiased train/test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:50.247914Z",
     "start_time": "2019-07-25T01:15:50.235284Z"
    }
   },
   "outputs": [],
   "source": [
    "regulons_with_noise.shape ##Let's look at the nrows and ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:15:54.629058Z",
     "start_time": "2019-07-25T01:15:54.537409Z"
    }
   },
   "outputs": [],
   "source": [
    "regulons_with_noise.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:16:56.817277Z",
     "start_time": "2019-07-25T01:16:41.627609Z"
    }
   },
   "outputs": [],
   "source": [
    "regulons_with_noise.to_csv('../data/ml_dfs/ecoli_ml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can divide our X and y data. X_data will be pure expression data, and y_data corresponds to the cluster labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.869096Z",
     "start_time": "2019-07-24T08:14:33.506Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data = regulons_with_noise.iloc[:,:-10]\n",
    "y_data = regulons_with_noise.iloc[:,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.871976Z",
     "start_time": "2019-07-24T08:14:33.512Z"
    }
   },
   "outputs": [],
   "source": [
    "clus2 = y_data['cluster 2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.874547Z",
     "start_time": "2019-07-24T08:14:33.517Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = SelectPercentile(f_classif, percentile=80).fit_transform(X_data, clus2)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a random partition from the `regulons_with_noise` data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.880761Z",
     "start_time": "2019-07-24T08:14:33.529Z"
    }
   },
   "outputs": [],
   "source": [
    "#The test subset will correspond to 30% of the data at random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.886404Z",
     "start_time": "2019-07-24T08:14:33.535Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split the data with the 80% most important PCs\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_data, test_size=0.2, random_state=42) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Neural Network using Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to train a neural network using Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.890282Z",
     "start_time": "2019-07-24T08:14:33.543Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output layer size = 10.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.893787Z",
     "start_time": "2019-07-24T08:14:33.550Z"
    }
   },
   "outputs": [],
   "source": [
    "#softmax activation\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1000, activation='softmax', input_dim=800))\n",
    "model.add(Dense(units=10))# 10 output\n",
    "model.compile(loss= 'mse', optimizer='RMSprop', metrics= ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.896326Z",
     "start_time": "2019-07-24T08:14:33.556Z"
    }
   },
   "outputs": [],
   "source": [
    "x = bokeh.plotting.figure(height=400,\n",
    "                          width=650,\n",
    "                          x_axis_label='epoch', \n",
    "                          y_axis_label='accuracy',\n",
    "                         y_range=(0, 1), title= 'Model Training Accuracy')\n",
    "\n",
    "x.circle(x = np.arange(1,101,1), y =history.history['acc'], fill_alpha = 0.5)\n",
    "#x.line(t, p[:,1])\n",
    "#x.line(t, p[:,2])\n",
    "bokeh.io.show(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.899783Z",
     "start_time": "2019-07-24T08:14:33.562Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.901964Z",
     "start_time": "2019-07-24T08:14:33.568Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras simulations using Matplotlib \n",
    "\n",
    "n_simulations = 30\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data\n",
    "                                                        , test_size=0.3, random_state=42) \n",
    "    \n",
    "    #softmax activation\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1000, activation='softmax', input_dim=n_components))\n",
    "    model.add(Dense(units=10))# 10 output\n",
    "    model.compile(loss= 'mse', optimizer='RMSprop', metrics= ['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=80, batch_size= 200)\n",
    "\n",
    "    accuracy = history.history['acc']\n",
    "    loss = history.history['loss']\n",
    "    train_acc.append(accuracy[79])\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test,verbose=0)\n",
    "    test_acc.append(score[1])\n",
    "\n",
    "    # summarize history for accuracy/loss\n",
    "    plt.plot(accuracy, 'o', color = 'royalblue', alpha = 0.3, markersize= 5)\n",
    "    plt.plot(loss, 'o', color = 'orangered', alpha = 0.3, markersize= 5)\n",
    "    plt.title('Keras Model Training $E. coli$ ', fontsize = 16)\n",
    "    plt.ylabel('Acc / Loss ')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(0,1.05)\n",
    "    plt.legend(['Acc.','Loss' ], loc='best')\n",
    "    \n",
    "plt.savefig('keras-model-train-ecoli.tiff', dpi = 350)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.904273Z",
     "start_time": "2019-07-24T08:14:33.574Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "organism = ['$E. coli$'] * len(train_acc)\n",
    "train = ['train'] * len(train_acc)\n",
    "test = ['test'] * len(train_acc)\n",
    "\n",
    "x = list(zip(train_acc, organism,train))\n",
    "y = list(zip(test_acc, organism, test))\n",
    "\n",
    "\n",
    "entries= x + y \n",
    "\n",
    "ecoli_df = pd.DataFrame(index = range(n_simulations*2))\n",
    "ecoli_df = pd.DataFrame(entries, columns=['accuracy', 'organism', 'type'])\n",
    "\n",
    "ecoli_df.to_csv('ecoli-model.csv')\n",
    "\n",
    "sns.violinplot(x = 'organism', y = 'accuracy', hue = 'type', data = ecoli_df,\n",
    "               inner = 'quartile',palette = 'pastel')\n",
    "\n",
    "\n",
    "plt.ylim(.3, 1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.907351Z",
     "start_time": "2019-07-24T08:14:33.580Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size =100)\n",
    "y_pred_flat = np.round(y_pred.flatten())\n",
    "y_test_flat = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.913158Z",
     "start_time": "2019-07-24T08:14:33.588Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.915612Z",
     "start_time": "2019-07-24T08:14:33.594Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['not in cluster', 'inside cluster'], yticklabels=['not in cluster', 'inside cluster'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Keras Classifier confusion matrix $E. coli$')\n",
    "#plt.savefig('conf-mat-ecoli-keras.tiff', dpi = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.919827Z",
     "start_time": "2019-07-24T08:14:33.605Z"
    }
   },
   "outputs": [],
   "source": [
    "#Relu activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2000, activation='relu', input_dim=n_components))\n",
    "model.add(Dense(units=10))# 11 outputs\n",
    "model.compile(loss='mse', optimizer='SGD', metrics= ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=80, batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.922952Z",
     "start_time": "2019-07-24T08:14:33.610Z"
    }
   },
   "outputs": [],
   "source": [
    "x = bokeh.plotting.figure(height=400,\n",
    "                          width=650,\n",
    "                          x_axis_label='epoch', \n",
    "                          y_axis_label='accuracy',\n",
    "                         y_range=(0, 1), title= 'Model Training Accuracy')\n",
    "\n",
    "x.circle(x = np.arange(1,81,1), y =history.history['acc'], fill_alpha = 0.5)\n",
    "#x.line(t, p[:,1])\n",
    "#x.line(t, p[:,2])\n",
    "bokeh.io.show(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.927422Z",
     "start_time": "2019-07-24T08:14:33.616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Softmax activation with 200 epochs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=500, activation='softmax', input_dim=n_components))\n",
    "model.add(Dense(units=10))# 10 output\n",
    "model.compile(loss='mse', optimizer= 'RMSprop', metrics= ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.933331Z",
     "start_time": "2019-07-24T08:14:33.622Z"
    }
   },
   "outputs": [],
   "source": [
    "x = bokeh.plotting.figure(height=400,\n",
    "                          width=650,\n",
    "                          x_axis_label='epoch', \n",
    "                          y_axis_label='accuracy',\n",
    "                         y_range=(0, 1), title= 'Model Training Accuracy')\n",
    "\n",
    "x.circle(x = np.arange(1,201,1), y =history.history['acc'], fill_alpha = 0.5)\n",
    "\n",
    "bokeh.io.show(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with other ML algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.935417Z",
     "start_time": "2019-07-24T08:14:33.629Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.937482Z",
     "start_time": "2019-07-24T08:14:33.634Z"
    }
   },
   "outputs": [],
   "source": [
    "perceptron = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(1200, 20), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.939906Z",
     "start_time": "2019-07-24T08:14:33.641Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = MultiOutputClassifier(perceptron)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred= clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.944962Z",
     "start_time": "2019-07-24T08:14:33.655Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.947925Z",
     "start_time": "2019-07-24T08:14:33.665Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.950061Z",
     "start_time": "2019-07-24T08:14:33.675Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pred = y_pred_df.apply(lambda x: x.idxmax(), axis = 1)\n",
    "df_pred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.953083Z",
     "start_time": "2019-07-24T08:14:33.683Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = y_test_df.apply(lambda x: x.idxmax(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.956180Z",
     "start_time": "2019-07-24T08:14:33.692Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.961343Z",
     "start_time": "2019-07-24T08:14:33.706Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_list = list(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.963758Z",
     "start_time": "2019-07-24T08:14:33.716Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.965960Z",
     "start_time": "2019-07-24T08:14:33.727Z"
    }
   },
   "outputs": [],
   "source": [
    "integer_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.968754Z",
     "start_time": "2019-07-24T08:14:33.735Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.972768Z",
     "start_time": "2019-07-24T08:14:33.748Z"
    }
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(integer_encoded, df_pred)\n",
    "np.set_printoptions(precision=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.978014Z",
     "start_time": "2019-07-24T08:14:33.767Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names = list(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.980969Z",
     "start_time": "2019-07-24T08:14:33.775Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,normalize = True,\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.983399Z",
     "start_time": "2019-07-24T08:14:33.789Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.988796Z",
     "start_time": "2019-07-24T08:14:33.797Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.991762Z",
     "start_time": "2019-07-24T08:14:33.807Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder.inverse_transform([argmax(onehot_encoded[2, :])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.995346Z",
     "start_time": "2019-07-24T08:14:33.834Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:16.999082Z",
     "start_time": "2019-07-24T08:14:33.841Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_flat = y_test.values.flatten()\n",
    "y_pred_flat = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.001387Z",
     "start_time": "2019-07-24T08:14:33.851Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['not in cluster', 'inside cluster'], yticklabels=['not in cluster', 'inside cluster'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('MLP Confusion Matrix $E. coli$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.003177Z",
     "start_time": "2019-07-24T08:14:33.863Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.006045Z",
     "start_time": "2019-07-24T08:14:33.875Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.008582Z",
     "start_time": "2019-07-24T08:14:33.961Z"
    }
   },
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = svm.SVC(kernel='linear', C=0.01)\n",
    "y_pred = classifier.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.011112Z",
     "start_time": "2019-07-24T08:14:33.992Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.013356Z",
     "start_time": "2019-07-24T08:14:34.018Z"
    }
   },
   "outputs": [],
   "source": [
    "perceptron.fit(X_train, y_train) \n",
    "\n",
    "y_pred= perceptron.predict(X_test)\n",
    "y_test_flat = y_test.values.flatten()\n",
    "y_pred_flat = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.016610Z",
     "start_time": "2019-07-24T08:14:34.041Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.019023Z",
     "start_time": "2019-07-24T08:14:34.054Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['not in cluster', 'inside cluster'], yticklabels=['not in cluster', 'inside cluster'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('MLP Confusion Matrix $E. coli$')\n",
    "plt.savefig('conf-mat-ecoli-mlp.tiff', dpi = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.022171Z",
     "start_time": "2019-07-24T08:14:34.064Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score : ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.025037Z",
     "start_time": "2019-07-24T08:14:34.074Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=30, max_depth=30, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_flat = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.027363Z",
     "start_time": "2019-07-24T08:14:34.088Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.029613Z",
     "start_time": "2019-07-24T08:14:34.098Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['not in cluster', 'inside cluster'], yticklabels=['not in cluster', 'inside cluster'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Random Forest Confusion Matrix $E. coli $')\n",
    "plt.savefig('conf-mat-ecoli-rf.tiff', dpi = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.031726Z",
     "start_time": "2019-07-24T08:14:34.117Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score : ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Keras model can make sense of highly noisy expression data, that might not represent the best dataset for training (i.e. it contains  conditions that might not be related to the TRN wiring per se, or the evolutionary history of $E.coli$ ). However, we get ~ 80% accuracy in the training. The next step would be to make the predictions, for each hypothetical TF to find out, what would be its functional module. However we will not jump and do that before we can exhaustively confirm that this is the best classification accuracy that we can get. \n",
    "\n",
    "\n",
    "It's important to emphasize that $E.coli$ might have some of these hypothetical TFs in low expression levels, and thus they might barely be exerting any significant regulation inside the cell, and still not represent an expensive genomic accesory in energetic levels (i.e. not wasting energy in transcription/translation). Thus, one possible explanation for the necessity for these TFs would be that they are a genomic arsenal to coordinate transcriptional programs for future events, that might confer an evolutionary advantage to the bacterium. In other words, these extended transcriptional repertoire might be an arsenal for future adverse conditions. However, this last suggestion has to be tested experimentally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I'll print out the versions of the most important Python modules used in the workflow for reproducibility purpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.033747Z",
     "start_time": "2019-07-24T08:14:34.154Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.036148Z",
     "start_time": "2019-07-24T08:14:34.168Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T08:17:17.038283Z",
     "start_time": "2019-07-24T08:14:34.189Z"
    }
   },
   "outputs": [],
   "source": [
    "print(keras.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(np.__version__)##numpy version\n",
    "print(nx.__version__)##NetworkX\n",
    "print(matplotlib.__version__)\n",
    "print(sns.__version__)#Seaborn\n",
    "print(pd.__version__)#Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
